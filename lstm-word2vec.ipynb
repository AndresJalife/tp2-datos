{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jorge/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jorge/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/jorge/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Embedding, GlobalMaxPooling1D, Dropout, LSTM,Input,Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "import feature_builder\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliteo para obtener vectores de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train,test = train_test_split(train_df,test_size=0.33,random_state = 17)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "y_train = train['target'].values\n",
    "y_test = test['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_cnn(df):\n",
    "    processed = feature_builder.process_dataset(df)\n",
    "    return (processed, processed.to_numpy().reshape(processed.shape[0], 1, processed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5100\n",
      "Percentage of words covered in the embeddings = 0.5339805825242718\n"
     ]
    }
   ],
   "source": [
    "processed, X_train = prepare_for_cnn(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text_embedding_0</th>\n",
       "      <th>text_embedding_1</th>\n",
       "      <th>text_embedding_2</th>\n",
       "      <th>text_embedding_3</th>\n",
       "      <th>text_embedding_4</th>\n",
       "      <th>text_embedding_5</th>\n",
       "      <th>text_embedding_6</th>\n",
       "      <th>text_embedding_7</th>\n",
       "      <th>text_embedding_8</th>\n",
       "      <th>...</th>\n",
       "      <th>text_embedding_290</th>\n",
       "      <th>text_embedding_291</th>\n",
       "      <th>text_embedding_292</th>\n",
       "      <th>text_embedding_293</th>\n",
       "      <th>text_embedding_294</th>\n",
       "      <th>text_embedding_295</th>\n",
       "      <th>text_embedding_296</th>\n",
       "      <th>text_embedding_297</th>\n",
       "      <th>text_embedding_298</th>\n",
       "      <th>text_embedding_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2247</td>\n",
       "      <td>0.174561</td>\n",
       "      <td>-0.187804</td>\n",
       "      <td>-0.174774</td>\n",
       "      <td>0.391602</td>\n",
       "      <td>-0.800507</td>\n",
       "      <td>1.263681</td>\n",
       "      <td>-0.333313</td>\n",
       "      <td>-0.530518</td>\n",
       "      <td>1.439468</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.093704</td>\n",
       "      <td>-0.026474</td>\n",
       "      <td>-1.963478</td>\n",
       "      <td>0.668640</td>\n",
       "      <td>0.470688</td>\n",
       "      <td>0.261047</td>\n",
       "      <td>0.084839</td>\n",
       "      <td>-0.743698</td>\n",
       "      <td>1.064865</td>\n",
       "      <td>-1.014771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6872</td>\n",
       "      <td>-0.832794</td>\n",
       "      <td>0.522888</td>\n",
       "      <td>1.680176</td>\n",
       "      <td>2.143860</td>\n",
       "      <td>-0.971680</td>\n",
       "      <td>-0.209106</td>\n",
       "      <td>0.139069</td>\n",
       "      <td>-3.171082</td>\n",
       "      <td>2.443481</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.377075</td>\n",
       "      <td>-0.090576</td>\n",
       "      <td>-1.615211</td>\n",
       "      <td>0.426344</td>\n",
       "      <td>-1.479126</td>\n",
       "      <td>-0.139343</td>\n",
       "      <td>-1.033325</td>\n",
       "      <td>-1.442383</td>\n",
       "      <td>1.079529</td>\n",
       "      <td>-0.524834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>0.658539</td>\n",
       "      <td>-0.151917</td>\n",
       "      <td>1.475098</td>\n",
       "      <td>1.222046</td>\n",
       "      <td>-0.955200</td>\n",
       "      <td>0.176971</td>\n",
       "      <td>0.430298</td>\n",
       "      <td>-1.353149</td>\n",
       "      <td>1.527988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303158</td>\n",
       "      <td>0.671021</td>\n",
       "      <td>-1.591064</td>\n",
       "      <td>0.691040</td>\n",
       "      <td>-0.726196</td>\n",
       "      <td>-0.286438</td>\n",
       "      <td>-0.898315</td>\n",
       "      <td>-0.965561</td>\n",
       "      <td>0.320984</td>\n",
       "      <td>-0.436462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>1.228882</td>\n",
       "      <td>0.942330</td>\n",
       "      <td>0.987427</td>\n",
       "      <td>0.798660</td>\n",
       "      <td>-1.113495</td>\n",
       "      <td>1.210213</td>\n",
       "      <td>0.925262</td>\n",
       "      <td>-0.703918</td>\n",
       "      <td>0.535461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.910240</td>\n",
       "      <td>0.496979</td>\n",
       "      <td>-1.143379</td>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.169312</td>\n",
       "      <td>-0.548679</td>\n",
       "      <td>-0.295044</td>\n",
       "      <td>-1.006866</td>\n",
       "      <td>-0.003662</td>\n",
       "      <td>-0.747009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5227</td>\n",
       "      <td>0.322754</td>\n",
       "      <td>0.200623</td>\n",
       "      <td>0.307800</td>\n",
       "      <td>0.546387</td>\n",
       "      <td>-0.006531</td>\n",
       "      <td>0.135468</td>\n",
       "      <td>-0.101074</td>\n",
       "      <td>-1.114014</td>\n",
       "      <td>1.345947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023804</td>\n",
       "      <td>-0.064453</td>\n",
       "      <td>-0.679199</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>-0.692871</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>-0.340820</td>\n",
       "      <td>-0.665649</td>\n",
       "      <td>-0.068527</td>\n",
       "      <td>-0.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>406</td>\n",
       "      <td>0.113220</td>\n",
       "      <td>0.679993</td>\n",
       "      <td>1.102051</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>-0.223755</td>\n",
       "      <td>0.406830</td>\n",
       "      <td>0.233826</td>\n",
       "      <td>-1.125244</td>\n",
       "      <td>1.049072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369751</td>\n",
       "      <td>-1.126282</td>\n",
       "      <td>-0.685760</td>\n",
       "      <td>1.176270</td>\n",
       "      <td>-0.364258</td>\n",
       "      <td>-0.521740</td>\n",
       "      <td>-0.972382</td>\n",
       "      <td>-1.240234</td>\n",
       "      <td>-0.127380</td>\n",
       "      <td>1.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5096</th>\n",
       "      <td>5510</td>\n",
       "      <td>-1.314148</td>\n",
       "      <td>0.022621</td>\n",
       "      <td>0.202881</td>\n",
       "      <td>1.220764</td>\n",
       "      <td>-1.881714</td>\n",
       "      <td>0.512215</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>-0.086838</td>\n",
       "      <td>2.091187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.970459</td>\n",
       "      <td>1.073730</td>\n",
       "      <td>0.985413</td>\n",
       "      <td>1.380371</td>\n",
       "      <td>-0.703445</td>\n",
       "      <td>-0.206177</td>\n",
       "      <td>0.860336</td>\n",
       "      <td>-0.740601</td>\n",
       "      <td>-0.217972</td>\n",
       "      <td>1.305298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5097</th>\n",
       "      <td>2191</td>\n",
       "      <td>-0.438354</td>\n",
       "      <td>-0.308868</td>\n",
       "      <td>-0.638916</td>\n",
       "      <td>0.773682</td>\n",
       "      <td>-0.996460</td>\n",
       "      <td>-0.459381</td>\n",
       "      <td>-0.472260</td>\n",
       "      <td>-2.293091</td>\n",
       "      <td>1.772034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197659</td>\n",
       "      <td>0.354160</td>\n",
       "      <td>-0.334763</td>\n",
       "      <td>0.917114</td>\n",
       "      <td>0.396240</td>\n",
       "      <td>-0.645752</td>\n",
       "      <td>-1.683411</td>\n",
       "      <td>-0.902588</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.130371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5098</th>\n",
       "      <td>7409</td>\n",
       "      <td>1.274780</td>\n",
       "      <td>1.120117</td>\n",
       "      <td>2.076721</td>\n",
       "      <td>-0.992424</td>\n",
       "      <td>1.082748</td>\n",
       "      <td>-0.778595</td>\n",
       "      <td>0.857422</td>\n",
       "      <td>-2.068481</td>\n",
       "      <td>0.129395</td>\n",
       "      <td>...</td>\n",
       "      <td>1.093018</td>\n",
       "      <td>-0.877655</td>\n",
       "      <td>-0.676270</td>\n",
       "      <td>0.183716</td>\n",
       "      <td>-0.272217</td>\n",
       "      <td>1.118835</td>\n",
       "      <td>-1.630981</td>\n",
       "      <td>-2.040894</td>\n",
       "      <td>1.220459</td>\n",
       "      <td>0.724030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5099</th>\n",
       "      <td>2671</td>\n",
       "      <td>-0.608093</td>\n",
       "      <td>1.031808</td>\n",
       "      <td>0.014771</td>\n",
       "      <td>1.548584</td>\n",
       "      <td>-0.077576</td>\n",
       "      <td>-0.075806</td>\n",
       "      <td>0.045441</td>\n",
       "      <td>-0.428223</td>\n",
       "      <td>0.823486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655762</td>\n",
       "      <td>-0.166138</td>\n",
       "      <td>-1.176613</td>\n",
       "      <td>1.351929</td>\n",
       "      <td>-0.160400</td>\n",
       "      <td>-0.811768</td>\n",
       "      <td>-1.258423</td>\n",
       "      <td>-1.154724</td>\n",
       "      <td>-0.114746</td>\n",
       "      <td>0.670532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5100 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  text_embedding_0  text_embedding_1  text_embedding_2  \\\n",
       "0      2247          0.174561         -0.187804         -0.174774   \n",
       "1      6872         -0.832794          0.522888          1.680176   \n",
       "2        82          0.658539         -0.151917          1.475098   \n",
       "3      1997          1.228882          0.942330          0.987427   \n",
       "4      5227          0.322754          0.200623          0.307800   \n",
       "...     ...               ...               ...               ...   \n",
       "5095    406          0.113220          0.679993          1.102051   \n",
       "5096   5510         -1.314148          0.022621          0.202881   \n",
       "5097   2191         -0.438354         -0.308868         -0.638916   \n",
       "5098   7409          1.274780          1.120117          2.076721   \n",
       "5099   2671         -0.608093          1.031808          0.014771   \n",
       "\n",
       "      text_embedding_3  text_embedding_4  text_embedding_5  text_embedding_6  \\\n",
       "0             0.391602         -0.800507          1.263681         -0.333313   \n",
       "1             2.143860         -0.971680         -0.209106          0.139069   \n",
       "2             1.222046         -0.955200          0.176971          0.430298   \n",
       "3             0.798660         -1.113495          1.210213          0.925262   \n",
       "4             0.546387         -0.006531          0.135468         -0.101074   \n",
       "...                ...               ...               ...               ...   \n",
       "5095          0.953125         -0.223755          0.406830          0.233826   \n",
       "5096          1.220764         -1.881714          0.512215         -0.020142   \n",
       "5097          0.773682         -0.996460         -0.459381         -0.472260   \n",
       "5098         -0.992424          1.082748         -0.778595          0.857422   \n",
       "5099          1.548584         -0.077576         -0.075806          0.045441   \n",
       "\n",
       "      text_embedding_7  text_embedding_8  ...  text_embedding_290  \\\n",
       "0            -0.530518          1.439468  ...           -3.093704   \n",
       "1            -3.171082          2.443481  ...           -1.377075   \n",
       "2            -1.353149          1.527988  ...           -0.303158   \n",
       "3            -0.703918          0.535461  ...           -0.910240   \n",
       "4            -1.114014          1.345947  ...            0.023804   \n",
       "...                ...               ...  ...                 ...   \n",
       "5095         -1.125244          1.049072  ...            0.369751   \n",
       "5096         -0.086838          2.091187  ...           -0.970459   \n",
       "5097         -2.293091          1.772034  ...           -0.197659   \n",
       "5098         -2.068481          0.129395  ...            1.093018   \n",
       "5099         -0.428223          0.823486  ...           -0.655762   \n",
       "\n",
       "      text_embedding_291  text_embedding_292  text_embedding_293  \\\n",
       "0              -0.026474           -1.963478            0.668640   \n",
       "1              -0.090576           -1.615211            0.426344   \n",
       "2               0.671021           -1.591064            0.691040   \n",
       "3               0.496979           -1.143379            0.590332   \n",
       "4              -0.064453           -0.679199            0.453125   \n",
       "...                  ...                 ...                 ...   \n",
       "5095           -1.126282           -0.685760            1.176270   \n",
       "5096            1.073730            0.985413            1.380371   \n",
       "5097            0.354160           -0.334763            0.917114   \n",
       "5098           -0.877655           -0.676270            0.183716   \n",
       "5099           -0.166138           -1.176613            1.351929   \n",
       "\n",
       "      text_embedding_294  text_embedding_295  text_embedding_296  \\\n",
       "0               0.470688            0.261047            0.084839   \n",
       "1              -1.479126           -0.139343           -1.033325   \n",
       "2              -0.726196           -0.286438           -0.898315   \n",
       "3               0.169312           -0.548679           -0.295044   \n",
       "4              -0.692871           -0.314453           -0.340820   \n",
       "...                  ...                 ...                 ...   \n",
       "5095           -0.364258           -0.521740           -0.972382   \n",
       "5096           -0.703445           -0.206177            0.860336   \n",
       "5097            0.396240           -0.645752           -1.683411   \n",
       "5098           -0.272217            1.118835           -1.630981   \n",
       "5099           -0.160400           -0.811768           -1.258423   \n",
       "\n",
       "      text_embedding_297  text_embedding_298  text_embedding_299  \n",
       "0              -0.743698            1.064865           -1.014771  \n",
       "1              -1.442383            1.079529           -0.524834  \n",
       "2              -0.965561            0.320984           -0.436462  \n",
       "3              -1.006866           -0.003662           -0.747009  \n",
       "4              -0.665649           -0.068527           -0.080078  \n",
       "...                  ...                 ...                 ...  \n",
       "5095           -1.240234           -0.127380            1.053711  \n",
       "5096           -0.740601           -0.217972            1.305298  \n",
       "5097           -0.902588            0.569000            0.130371  \n",
       "5098           -2.040894            1.220459            0.724030  \n",
       "5099           -1.154724           -0.114746            0.670532  \n",
       "\n",
       "[5100 rows x 301 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    model = Sequential([\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64),input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 128)               187392    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 195,713\n",
      "Trainable params: 195,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3705 - accuracy: 0.8319 - val_loss: 0.5131 - val_accuracy: 0.7931\n",
      "Epoch 2/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3667 - accuracy: 0.8336 - val_loss: 0.5014 - val_accuracy: 0.7922\n",
      "Epoch 3/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3731 - accuracy: 0.8289 - val_loss: 0.5099 - val_accuracy: 0.7922\n",
      "Epoch 4/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3604 - accuracy: 0.8363 - val_loss: 0.5255 - val_accuracy: 0.7902\n",
      "Epoch 5/50\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3620 - accuracy: 0.8395 - val_loss: 0.5257 - val_accuracy: 0.7873\n",
      "Epoch 6/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3635 - accuracy: 0.8348 - val_loss: 0.5091 - val_accuracy: 0.8000\n",
      "Epoch 7/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3625 - accuracy: 0.8368 - val_loss: 0.5074 - val_accuracy: 0.8020\n",
      "Epoch 8/50\n",
      "128/128 [==============================] - 2s 13ms/step - loss: 0.3594 - accuracy: 0.8387 - val_loss: 0.5267 - val_accuracy: 0.7843\n",
      "Epoch 9/50\n",
      "128/128 [==============================] - 2s 12ms/step - loss: 0.3536 - accuracy: 0.8471 - val_loss: 0.5177 - val_accuracy: 0.7941\n",
      "Epoch 10/50\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.3565 - accuracy: 0.8429 - val_loss: 0.5216 - val_accuracy: 0.7902\n",
      "Epoch 11/50\n",
      "128/128 [==============================] - 2s 17ms/step - loss: 0.3588 - accuracy: 0.8365 - val_loss: 0.5230 - val_accuracy: 0.7873\n",
      "Epoch 12/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3661 - accuracy: 0.8336 - val_loss: 0.5268 - val_accuracy: 0.7961\n",
      "Epoch 13/50\n",
      "128/128 [==============================] - 1s 10ms/step - loss: 0.3559 - accuracy: 0.8402 - val_loss: 0.5339 - val_accuracy: 0.7902\n",
      "Epoch 14/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3613 - accuracy: 0.8355 - val_loss: 0.5337 - val_accuracy: 0.7922\n",
      "Epoch 15/50\n",
      "128/128 [==============================] - 3s 21ms/step - loss: 0.3519 - accuracy: 0.8397 - val_loss: 0.5292 - val_accuracy: 0.7882\n",
      "Epoch 16/50\n",
      "128/128 [==============================] - 2s 13ms/step - loss: 0.3532 - accuracy: 0.8409 - val_loss: 0.5262 - val_accuracy: 0.7922\n",
      "Epoch 17/50\n",
      "128/128 [==============================] - 2s 14ms/step - loss: 0.3507 - accuracy: 0.8429 - val_loss: 0.5302 - val_accuracy: 0.7912\n",
      "Epoch 18/50\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.3448 - accuracy: 0.8473 - val_loss: 0.5282 - val_accuracy: 0.7951\n",
      "Epoch 19/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3454 - accuracy: 0.8456 - val_loss: 0.5383 - val_accuracy: 0.7824\n",
      "Epoch 20/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3429 - accuracy: 0.8436 - val_loss: 0.5300 - val_accuracy: 0.7971\n",
      "Epoch 21/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3551 - accuracy: 0.8431 - val_loss: 0.5220 - val_accuracy: 0.7863\n",
      "Epoch 22/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3446 - accuracy: 0.8446 - val_loss: 0.5344 - val_accuracy: 0.7971\n",
      "Epoch 23/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3391 - accuracy: 0.8510 - val_loss: 0.5359 - val_accuracy: 0.7902\n",
      "Epoch 24/50\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.3491 - accuracy: 0.8451 - val_loss: 0.5762 - val_accuracy: 0.7578\n",
      "Epoch 25/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3537 - accuracy: 0.8407 - val_loss: 0.5369 - val_accuracy: 0.7980\n",
      "Epoch 26/50\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.3453 - accuracy: 0.8426 - val_loss: 0.5321 - val_accuracy: 0.7892\n",
      "Epoch 27/50\n",
      "128/128 [==============================] - 1s 11ms/step - loss: 0.3435 - accuracy: 0.8436 - val_loss: 0.5446 - val_accuracy: 0.7961\n",
      "Epoch 28/50\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3440 - accuracy: 0.8449 - val_loss: 0.5428 - val_accuracy: 0.7971\n",
      "Epoch 29/50\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3363 - accuracy: 0.8485 - val_loss: 0.5465 - val_accuracy: 0.7961\n",
      "Epoch 30/50\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3341 - accuracy: 0.8527 - val_loss: 0.5426 - val_accuracy: 0.7961\n",
      "Epoch 31/50\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8542 - val_loss: 0.5681 - val_accuracy: 0.7882\n",
      "Epoch 32/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3419 - accuracy: 0.8468 - val_loss: 0.5596 - val_accuracy: 0.7843\n",
      "Epoch 33/50\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3453 - accuracy: 0.8466 - val_loss: 0.5467 - val_accuracy: 0.7951\n",
      "Epoch 34/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3310 - accuracy: 0.8517 - val_loss: 0.5676 - val_accuracy: 0.7843\n",
      "Epoch 35/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3368 - accuracy: 0.8483 - val_loss: 0.5530 - val_accuracy: 0.7912\n",
      "Epoch 36/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3323 - accuracy: 0.8529 - val_loss: 0.5500 - val_accuracy: 0.7971\n",
      "Epoch 37/50\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3283 - accuracy: 0.8547 - val_loss: 0.5614 - val_accuracy: 0.7951\n",
      "Epoch 38/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3268 - accuracy: 0.8559 - val_loss: 0.5510 - val_accuracy: 0.7961\n",
      "Epoch 39/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3237 - accuracy: 0.8561 - val_loss: 0.5757 - val_accuracy: 0.7912\n",
      "Epoch 40/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3240 - accuracy: 0.8569 - val_loss: 0.5711 - val_accuracy: 0.7784\n",
      "Epoch 41/50\n",
      "128/128 [==============================] - 1s 8ms/step - loss: 0.3306 - accuracy: 0.8539 - val_loss: 0.5847 - val_accuracy: 0.7784\n",
      "Epoch 42/50\n",
      "128/128 [==============================] - 1s 6ms/step - loss: 0.3201 - accuracy: 0.8581 - val_loss: 0.5719 - val_accuracy: 0.7912\n",
      "Epoch 43/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3248 - accuracy: 0.8588 - val_loss: 0.5684 - val_accuracy: 0.7990\n",
      "Epoch 44/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3231 - accuracy: 0.8547 - val_loss: 0.5810 - val_accuracy: 0.7863\n",
      "Epoch 45/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3215 - accuracy: 0.8561 - val_loss: 0.5721 - val_accuracy: 0.7980\n",
      "Epoch 46/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3170 - accuracy: 0.8615 - val_loss: 0.5649 - val_accuracy: 0.8010\n",
      "Epoch 47/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3157 - accuracy: 0.8615 - val_loss: 0.5771 - val_accuracy: 0.7922\n",
      "Epoch 48/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3215 - accuracy: 0.8566 - val_loss: 0.5752 - val_accuracy: 0.7961\n",
      "Epoch 49/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3226 - accuracy: 0.8544 - val_loss: 0.5733 - val_accuracy: 0.7961\n",
      "Epoch 50/50\n",
      "128/128 [==============================] - 1s 7ms/step - loss: 0.3114 - accuracy: 0.8627 - val_loss: 0.6003 - val_accuracy: 0.7912\n"
     ]
    }
   ],
   "source": [
    "#es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=5)\n",
    "history = model.fit(X_train, y_train, epochs=50, shuffle=True, validation_split=0.2)#, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2513\n",
      "Percentage of words covered in the embeddings = 0.6020887228130689\n"
     ]
    }
   ],
   "source": [
    "processed, final_test = prepare_for_cnn(test)\n",
    "prediction = model.predict_classes(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8101870274572225"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test['target'], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7557603686635944"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test['target'], prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
