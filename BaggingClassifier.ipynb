{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>Bagging Classifier</strong> </h1> \n",
    "    \n",
    "<h3>Acá se va a estar probando el ensamblaje de Bagging con los distintos algoritmos implementados.</h3>\n",
    "\n",
    "Primero se hacen los imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from Exporter.ipynb\n",
      "Importing Jupyter notebook from hyperparameter_tunning.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nbimporter # pip install nbimporter\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import svm\n",
    "from catboost import CatBoostClassifier # pip install catboost\n",
    "from lightgbm import LGBMClassifier  #pip install lightgbm\n",
    "from xgboost import XGBClassifier #pip install xgboost\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "from feature_builder import process_dataset\n",
    "from Exporter import export_model_csv\n",
    "from hyperparameter_tunning import obtener_hiperparametros\n",
    "\n",
    "hyper_params = obtener_hiperparametros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_dataset.loc[:,'target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparo los distintos sets con features diferentes.</h2>\n",
    "Los distintos algoritmos necesitan diferentes sets, según lo investigado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Primero los procesados completos con spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words covered in the embeddings = 0.4937444933920705\n",
      "Percentage of words covered in the embeddings = 0.5961027457927369\n"
     ]
    }
   ],
   "source": [
    "x_processed = process_dataset(train_dataset, use_spacy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_processed, x_test_processed, y_train_processed, y_test_processed = train_test_split(x_processed, y, test_size = .30, random_state = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Los que solo necesitan embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded!\n",
      "Percentage of words covered in the embeddings = 0.4937444933920705\n",
      "Embeddings loaded!\n",
      "Percentage of words covered in the embeddings = 0.5961027457927369\n"
     ]
    }
   ],
   "source": [
    "x_embedd = process_dataset(train_dataset, use_spacy=True, use_manual_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embedd, x_test_embedd, y_train_embedd, y_test_embedd = train_test_split(x_embedd, y, test_size = .30, random_state = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Los que usan TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = TfidfVectorizer()\n",
    "x_tfidf = v.fit_transform(train_dataset['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tfidf, x_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(x_tfidf, y, test_size = .33, random_state = 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression(solver='liblinear', penalty='l1', multi_class='auto', max_iter=1000, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_LR = BaggingClassifier(base_estimator= logisticRegr, n_estimators=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1, class_weight=None,\n",
       "                                                    dual=False,\n",
       "                                                    fit_intercept=True,\n",
       "                                                    intercept_scaling=1,\n",
       "                                                    l1_ratio=None,\n",
       "                                                    max_iter=1000,\n",
       "                                                    multi_class='auto',\n",
       "                                                    n_jobs=None, penalty='l1',\n",
       "                                                    random_state=None,\n",
       "                                                    solver='liblinear',\n",
       "                                                    tol=0.0001, verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BC_LR.fit(x_train_processed, y_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_LR = BC_LR.predict(x_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7899073120494334"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_processed, y_predict_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(kernel='rbf', gamma='scale', degree=9, coef0=5, C=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_SVM = BaggingClassifier(base_estimator= SVM, n_estimators=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                     class_weight=None, coef0=0.0,\n",
       "                                     decision_function_shape='ovr', degree=3,\n",
       "                                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                     probability=False, random_state=None,\n",
       "                                     shrinking=True, tol=0.001, verbose=False),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BC_SVM.fit(x_train_embedd, y_train_embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = BC_SVM.predict(x_test_embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78082929456112"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_embedd, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[1152  138]\n",
      " [ 269  725]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix : \\n' + str(confusion_matrix(y_test_embedd,y_pred_svm))) #ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost = CatBoostClassifier(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_CB = BaggingClassifier(base_estimator= catboost, n_estimators=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=<catboost.core.CatBoostClassifier object at 0x7f023b63a310>,\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BC_CB.fit(x_train_processed, y_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_processed = BC_CB.predict(x_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.781897491821156"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_processed, y_pred_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MNB\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiNB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_MNB = BaggingClassifier(base_estimator= MultiNB, n_estimators=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                               fit_prior=True),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BC_MNB.fit(x_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MNB = BC_MNB.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7233351380617218"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_tfidf, y_pred_MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbooster = XGBClassifier(max_depth=3, n_estimators=600, colsample_bytree=0.9,\n",
    "                        subsample=0.9, nthread=4, learning_rate=0.05)\n",
    "\n",
    "\n",
    "BC_XGB = BaggingClassifier(base_estimator= xgbooster, n_estimators=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                               colsample_bylevel=None,\n",
       "                                               colsample_bynode=None,\n",
       "                                               colsample_bytree=0.9, gamma=None,\n",
       "                                               gpu_id=None,\n",
       "                                               importance_type='gain',\n",
       "                                               interaction_constraints=None,\n",
       "                                               learning_rate=0.05,\n",
       "                                               max_delta_step=None, max_depth=3,\n",
       "                                               min_child_weight=None,\n",
       "                                               missing=nan,\n",
       "                                               monotone_constraints=None,\n",
       "                                               n_estimators=600, n_jobs=None,\n",
       "                                               nthread=4,\n",
       "                                               num_parallel_tree=None,\n",
       "                                               random_state=None,\n",
       "                                               reg_alpha=None, reg_lambda=None,\n",
       "                                               scale_pos_weight=None,\n",
       "                                               subsample=0.9, tree_method=None,\n",
       "                                               validate_parameters=None,\n",
       "                                               verbosity=None),\n",
       "                  random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BC_XGB.fit(x_train_processed, y_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_processed = BC_XGB.predict(x_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7902964959568733"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_processed, y_pred_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LGBMClassifier(), random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = LGBMClassifier()\n",
    "\n",
    "BC_LGB = BaggingClassifier(base_estimator= gbm, n_estimators=10, random_state=0)\n",
    "BC_LGB.fit(x_train_processed, y_train_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_processed = BC_LGB.predict(x_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7810972297664313"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_processed, y_pred_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Paso a exportar el mejor resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded!\n",
      "Percentage of words covered in the embeddings = 0.4937444933920705\n",
      "Embeddings loaded!\n",
      "Percentage of words covered in the embeddings = 0.5961027457927369\n"
     ]
    }
   ],
   "source": [
    "x_train_final = process_dataset(train_dataset, use_spacy=True, use_manual_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded!\n",
      "Percentage of words covered in the embeddings = 0.5707598689343111\n",
      "Embeddings loaded!\n",
      "Percentage of words covered in the embeddings = 0.665389037945573\n"
     ]
    }
   ],
   "source": [
    "x_test_final = process_dataset(test_dataset, use_spacy=True, use_manual_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model_csv(BC_SVM, x_train_final, x_test_final, test_dataset, y, 'BC-SVC-spacy-no-feat-hyp-tun')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
