{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roBERTa",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXExTMpJI6GX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "3f406037-75a5-460e-c261-afc603bdd901"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████▍                   | 296kB 3.4MB/s eta 0:00:01"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuZTuSlG5sy5",
        "colab_type": "text"
      },
      "source": [
        "Importamos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lznNXie3I95i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import util\n",
        "import gc\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Embedding, GlobalMaxPooling1D, Dropout, Input\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from transformers import RobertaTokenizer, RobertaConfig, TFRobertaPreTrainedModel\n",
        "from transformers.modeling_tf_roberta import TFRobertaMainLayer\n",
        "from transformers.modeling_tf_utils import get_initializer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_bs-CUb5nZZ",
        "colab_type": "text"
      },
      "source": [
        "Cargamos los datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWVqCjeII-FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('train.csv', dtype={'id': np.int16, 'target': np.int8})\n",
        "test_df = pd.read_csv('test.csv', dtype={'id': np.int16, 'target': np.int8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmhRwYYl5gsN",
        "colab_type": "text"
      },
      "source": [
        "Activamos el uso de TPU en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--BkDb3SMy-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34pNaXr65yI5",
        "colab_type": "text"
      },
      "source": [
        "Definimos roBERTa usando la librearia transformers de huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivnVNQ5YJCoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class roBERTaModel(TFRobertaPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(roBERTaModel, self).__init__(config, *inputs, **kwargs)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.classifier = tf.keras.layers.Dense(units=config.num_labels, name='classifier', kernel_initializer=get_initializer(config.initializer_range))\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        outputs = self.roberta(inputs, **kwargs)\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout_1(pooled_output, training=kwargs.get('training', False))\n",
        "        logits = self.classifier(pooled_output)\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class roBERTaClassifier():\n",
        "    \n",
        "    def __init__(self, max_seq_length, lr, epochs, batch_size, splits):\n",
        "        self.model_name = 'roberta-base'\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.splits = splits\n",
        "        \n",
        "        \n",
        "    def encode(self, text_column):\n",
        "\n",
        "      def tokenize(x):\n",
        "        return self.tokenizer.encode_plus(x, max_length=self.max_seq_length, pad_to_max_length=True, truncation=True)\n",
        "\n",
        "      output = text_column.apply(lambda x: tokenize(x))\n",
        "      input_ids = np.array([feature['input_ids'] for feature in output])\n",
        "      masks = np.array([feature['attention_mask'] for feature in output])\n",
        "\n",
        "      return (input_ids, masks)\n",
        "    \n",
        "    \n",
        "    def _build_model(self):\n",
        "\n",
        "      with tpu_strategy.scope():\n",
        "          config = RobertaConfig.from_pretrained(self.model_name, num_labels=2)\n",
        "          model = roBERTaModel.from_pretrained(self.model_name)\n",
        "          optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
        "          loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "          metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
        "          model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "      return model\n",
        "    \n",
        "    \n",
        "    def train_and_predict(self, df, test):\n",
        "        model_count = self.splits\n",
        "        X_test_encoded = self.encode(test['text'])\n",
        "        y_pred = np.zeros((1, X_test_encoded[0].shape[0], 2))\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=model_count, random_state=None, shuffle=False)\n",
        "        for fold, (trn_idx, val_idx) in enumerate(skf.split(df['text'], df['target'])):\n",
        "            \n",
        "            print('\\nFold {}\\n'.format(fold))\n",
        "        \n",
        "            model = self._build_model()\n",
        "            X_trn = df.loc[trn_idx, 'text']\n",
        "            X_val = df.loc[val_idx, 'text']\n",
        "\n",
        "            X_trn_encoded = self.encode(X_trn)\n",
        "            y_trn = df.loc[trn_idx, 'target'].values.reshape(-1, 1)\n",
        "            X_val_encoded = self.encode(X_val)\n",
        "            y_val = df.loc[val_idx, 'target'].values.reshape(-1, 1)\n",
        "\n",
        "            y_trn_encoded, y_val_encoded = tf.keras.utils.to_categorical(y_trn), tf.keras.utils.to_categorical(y_val)\n",
        "\n",
        "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)]\n",
        "\n",
        "            history = model.fit([X_trn_encoded[0], X_trn_encoded[1]], y_trn_encoded, validation_data=([X_val_encoded[0], X_val_encoded[1]], y_val_encoded), epochs=self.epochs, batch_size=self.batch_size, callbacks=callbacks)\n",
        "            util.plot_history(history)\n",
        "\n",
        "            y_pred += model.predict(X_test_encoded)\n",
        "            del model\n",
        "            gc.collect()\n",
        "\n",
        "        return y_pred / model_count\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O96UocA-KxVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "roBERTa = roBERTaClassifier(max_seq_length=128, lr=3e-5, epochs=10, batch_size=128, splits=2)\n",
        "y_pred = roBERTa.train_and_predict(train_df, test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEH1WmMpctc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqiiZRMocu1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df = pd.read_csv('sample_submission.csv')\n",
        "final_df['target'] = np.argmax(y_pred[0], axis=1).flatten()\n",
        "final_df['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhYlNugxs0rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df.to_csv('roBERTa.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}